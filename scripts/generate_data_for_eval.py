#!/usr/bin/env python
# coding: utf-8


# The following script assumes you have in the same folder:
# * transcriptionsSAE.txt: file with all the possible correct pronunciations considered by the different alignment dictionaries.
# * trans_complete.txt: similar to transcriptionsSAE.txt but with more detailed annotations. Serves to compute labels.
# * annot_manual: directory with subdirectories containing TextGrid files with manual annotations for Epa-DB.
# * gop.1.txt generated by running the gop recipe for Epa-DB.
# * phones-pure.txt: file thar matches phone ints to phone symbols.

# ----------------------------------------------------------------------------

import os, errno, re
import os.path as path
from os import remove
import numpy as np
import textgrids # pip install git+https://github.com/Legisign/Praat-textgrids
from scipy.stats.stats import pearsonr
from IPython import embed
import pandas as pd
import joblib
import shutil
import argparse
import glob

def phonelist2str(phones):
    return " ".join(["%3s"%p for p in phones])

# Function that matches phone ints to phone symbols and loads them to a dictionary
def phones2dic(path):
    phones_dic = {}
    with open(path, "r") as fileHandler:
        line = fileHandler.readline()
        while line:
            l=line.split()
            phones_dic[int(l[1])] = l[0]
            line = fileHandler.readline()

    return phones_dic


def mkdirs(newdir):
    try: os.makedirs(newdir)
    except OSError as err:
        # Raise the error unless it's about an already existing directory
        if err.errno != errno.EEXIST or not os.path.isdir(newdir):
            raise

# Function that reads transcriptions files and loads them to
# a series of useful dictionaries
def generate_dict_from_transcripctions(transcriptions):

    trans_dict = dict()
    trans_dict_clean = dict()
    sent_dict = dict()

    # Read transcription file
    for line in open(transcriptions,'r'):

        fields = line.strip().split()

        if len(fields) <= 2:
            continue

        sent = fields[1].strip(":")

        if fields[0] == "TEXT":
            sent_dict[sent] = fields[2:]

        if fields[0] != "TRANSCRIPTION":
            continue

        if sent not in trans_dict_clean:
            # Before loading the first transcription for a sentence,
            # create an entry for it in the dict. The entry will be a
            # list of lists. One list for each possible transcription
            # for that sentence.
            trans_dict[sent] = list()
            trans_dict_clean[sent] = list()

        trans = [[]]
        for i in range(2, len(fields)):
            phones = fields[i].split("/")

            # Reproduce the transcriptions up to now as many times as
            # the number of phone variations in this slot. Then, append
            # one variation to each copy.
            trans_new = []
            for p in phones:
                for t in trans:
                    t_tmp = t + [p.strip()]
                    trans_new.append(t_tmp)
            trans = trans_new

        trans_dict[sent] += trans

    for sent, trans in trans_dict.items():
        trans_clean_new = []
        for t in trans:
            trans_clean_new.append([x for x in t if x != '0'])

        if sent not in trans_dict_clean:
            trans_dict_clean[sent] = list()

        trans_dict_clean[sent] += trans_clean_new

    return trans_dict, trans_dict_clean, sent_dict


# Function that reads the output of gop-dnn and returns the
# phone alignments

def get_gop_alignments(path_filename, phone_pure_dic):

    output = []
    for line in open(path_filename).readlines():
        l=line.split()

        if len(l) < 2:
            print("Invalid line")
        else:
            logid = l[0].replace("'", "")
            data = l[1:]
            i = 0
            phones = []
            gops = []
            phones_name = []
            while i < len(data):
                if data[i] == "[":
                    phone = int(data[i+1])
                    phone_name = phone_pure_dict[phone]

                    if phone_name not in ('SIL', 'sil', 'sp', 'spn', 'SP', 'SPN'):
                        gop = float(data[i+2])
                        phones.append(phone)
                        gops.append(gop)
                        phones_name.append(phone_name)


                    i = i + 4

            output.append({'logid': str(logid),
                            'phones': phones,
                           'phones_name':phones_name,
                           'gops':gops})

    df_phones = pd.DataFrame(output).set_index("logid")

    return df_phones


# Function that matches labels and phones from manual annotations with phones and scores from gop-dnn.
# This is a necessary step because manual annotations based on the forced alignments
# do not always coincide with gop's alignments.
# Note that whenever a phone is missing in the gop alignment a "?" is added to discard the corresponding label
# and, whenever a '0' (deletion) is present in the manual annotation, the gop score is discarded.

def match_labels2gop(logid, trans_zero, trans_manual, trans_auto, labels, gop_scores):

    output_ = []
    rows = []
    j = 0

    for i in range(0, len(trans_manual)):

        label = 0
        if(labels[i] == '+'):
            label = 1

        phone_manual = trans_manual[i]
        phone_zero = trans_zero[i]

        if phone_zero != '0':
            if j > len(trans_auto)-1:
                raise Exception("Index out of range")

            phone_automatic = trans_auto[j]
            rows.append([logid, phone_automatic, label, gop_scores[j]])
            j = j + 1

    columns = ['logid', 'phone', 'label', 'gop_scores']
    df = pd.DataFrame(rows, columns=columns)
    return df


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--trans-complete-file', dest='trans_complete_path', help='File with phonetic transcriptions of each of the phrases', default=None)
    parser.add_argument('--trans-SAE-file', dest='trans_SAE_path', help='File with phonetic transcriptions of each of the phrases', default=None)
    parser.add_argument('--labels-dir', dest='labels_dir', help='Directory with textgrid files with annotations', default=None)
    parser.add_argument('--output-dir', dest='output_dir', help='Output dir', default=None)
    parser.add_argument('--gop-file', dest='gop_path', help='File with gop results', default=None)
    parser.add_argument('--phones-pure-file', dest='phones_pure_path', help='file that matches phone ints to phone symbols', default=None)

    args = parser.parse_args()

    # Code that generates a pickle with useful data to analyze.
    # The outpul will be used to compute ROCs, AUCs and EERs.

    output = []

    trans_dict, trans_dict_clean, sent_dict = generate_dict_from_transcripctions(args.trans_SAE_path)
    trans_dict_complete, trans_dict_clean_complete, sent_dict_complete = generate_dict_from_transcripctions(args.trans_complete_path)

    phone_pure_dict = phones2dic(args.phones_pure_path)
    gop_alignments = get_gop_alignments(args.gop_path, args.phones_pure_path)

    utterance_list = [re.sub('.TextGrid','', re.sub('.*\/','',s)) for s in glob.glob("%s/*/*"%args.labels_dir)]

    # Now, iterate over utterances
    for utterance in utterance_list:

        spk, sent = utterance.split("_")
        tgfile = "%s/%s/%s.TextGrid"%(args.labels_dir, spk, utterance) #TextGrid file for current utterance

        print("----------------------------------------------------------------------------------------")
        print("Speaker %s, sentence %s: %s (File: %s)"%(spk, sent, " ".join(sent_dict[sent]), tgfile))

        try:
            tg = textgrids.TextGrid(tgfile)
        except:
            #continue
            raise Exception("Bad textgrid file %s"%tgfile)

        if len(tg) < 4:
            #continue
            raise Exception("WARNING: File %s does not have an annotation or a score tier\n"%tgfile)

        if utterance in gop_alignments.index.values:
            phone_idxs = gop_alignments.loc[utterance].phones
            gop_scores = gop_alignments.loc[utterance].gops
            annot_kaldi = []
            for phone_idx in phone_idxs:
                phone = phone_pure_dict[phone_idx]
                if phone not in ['sil', '[key]', 'sp', '', 'SIL', '[KEY]', 'SP']:
                    if phone[-1] not in ['1', '0', '2']:
                        annot_kaldi += [phone]
                    else:
                        # If it has an int at the end, delete it, except for AH0
                        annot_kaldi += [phone] if(phone == 'AH0') else [phone[:-1]]
        else:
            #continue
            raise Exception("WARNING: Missing score for "+ utterance)

        #Get phone list from manual annotation in current textgrid
        annot_manual = []

        for i in tg['annotation']:
            p = i.text.strip()
            if p not in ['sil', '', 'sp', 'None']:
                if p[-1] not in ['0','1', '2']:
                    annot_manual += [p]
                else:
                    annot_manual += [p]  if p == 'AH0' else [p[:-1]]
        #embed()
        # Find the transcription for this sentence that best matches the annotation
        best_trans = -1
        best_trans_corr = 0
        for trans_idx, trans in enumerate(trans_dict_clean[sent]):
            if(len(trans) == len(annot_kaldi)):
                num_correct = np.sum([t==a for t, a in np.c_[trans,annot_kaldi]])
                if num_correct > best_trans_corr:
                    best_trans_corr = num_correct
                    best_trans = trans_idx

            #else:
            #    print(trans)
            #    print(annot_manual)
            #    #continue
            #    raise Exception("Transcription length does not match annotation length (%d != %d)"%(len(trans), len(annot)))

        best_trans1 = -1
        best_trans_corr = 0
        for trans_idx, trans1 in enumerate(trans_dict_complete[sent]):
            if(len(trans1) == len(annot_manual)):
                num_correct = np.sum([t==a for t, a in np.c_[trans1,annot_manual]])
                if num_correct > best_trans_corr:
                    best_trans_corr = num_correct
                    best_trans1 = trans_idx
        #embed()

            #else:
            #    #continue
            #    raise Exception("Transcription length does not match annotation length (%d != %d)"%(len(trans1), len(annot_manual)))

        trans      = trans_dict_clean[sent][best_trans]
        trans_zero = trans_dict[sent][best_trans]
        trans_reff_complete = trans_dict_complete[sent][best_trans1]

        labels = np.array(['+' if t==a else '-' for t, a in np.c_[trans_reff_complete,annot_manual]])
        print("TRANS_REFF:           %s (chosen out of %d transcriptions)"%(phonelist2str(trans), len(trans_dict_clean[sent])))
        print("TRANS_KALDI:          "+phonelist2str(annot_kaldi))
        print("LABEL:                "+phonelist2str(labels))
        print("TRANS_ZERO:           "+phonelist2str(trans_zero))
        print("TRANS_MANUAL:         "+phonelist2str(annot_manual))
        print("TRANS_REFF_COMPLETE:  "+phonelist2str(trans_reff_complete))
        print("TRANS_WITHOUT_ZERO:   "+phonelist2str(trans))

        outdir  = "%s/labels/%s" % (args.output_dir, spk)
        outfile = "%s/%s.txt" % (outdir, utterance)
        mkdirs(outdir + '/labels')
        np.savetxt(outfile, np.c_[np.arange(len(annot_manual)), trans_reff_complete, annot_manual, labels], fmt=utterance+"_%s %s %s %s")

        df = match_labels2gop(utterance, trans_zero, annot_manual, annot_kaldi, labels, gop_scores)
        output.append(df)

    df_trans_match = pd.concat(output).set_index('logid')

    #Export file containing data for evaluation
    joblib.dump(df_trans_match, args.output_dir + '/data_for_eval.pickle')
